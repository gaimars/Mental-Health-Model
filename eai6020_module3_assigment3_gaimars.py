# -*- coding: utf-8 -*-
"""EAI6020_Module3_Assigment3_GaimarS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/124m4IPzD-izqsNPQoyaxoB0HaGkAwY80
"""

#setup
!pip install -Uqq fastbook
import fastbook
fastbook.setup_book()
from fastbook import *
from fastai.text.all import *

#locating the mental health dataset on drive and assigning it to a path
path = "/content/gdrive/MyDrive/EAI6020 - Quarter5/Assignment 3/Combined Data.csv"

# loading required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import re
import string
import torch
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

import textwrap

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import RegexpTokenizer

from xgboost import XGBClassifier

from transformers import BertTokenizer, BertModel

import warnings
warnings.filterwarnings("ignore")

"""# **Data Exploration and Data Cleaning**"""

# Read the CSV file and display first few records
df = pd.read_csv(path, index_col=0)
print(df.head())

# exploring the dataset and analyzing missing rows

print(df.shape)
print(df.info())
print(df.isnull().sum())

# Display the missing rows
print(df.isnull)

# since classification is not possible without the statement, we will delete these
# Remove rows where 'statement' column has missing values
df = df.dropna(subset=['statement'])

#Reconducting missing value analysis to confirm deletion
print(df.shape)
print(df.info())
print(df.isnull().sum())
print(df.describe())

# clean text
def clean_text(raw_text):
# Step 1: Remove HTML tags
  text = re.sub(r'<.*?>', '', raw_text)
# Step 2: Remove URLs
  text = re.sub(r'http\S+|www\S+|https\S+', '', text)
# Step 3: Remove special characters and digits
  text = re.sub(r'[^A-Za-z\s]', '', text)
# Step 4: Remove extra whitespaces
  text = re.sub(r'\s+', ' ', text).strip()
# Step 5: Convert to lowercase
  text = text.lower()
  return text
raw_text= df['statement']

#converting text to lowercase
df['statement'] = df['statement'].str.lower()

#cleaning the text
df['statement'] = df['statement'].apply(clean_text)

#removing stopwords
stopWords = stopwords.words('english')
df['statement'] = df['statement'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopWords)]))

df.head()

# Visualize status of different mental health conditions.

from matplotlib import pyplot as plt
import seaborn as sns
df.groupby('status').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

# observing different status types and their counts
df.status.value_counts()

#displaying one example statement from each status type
unique_status_example = df.groupby('status').first()

#displaying hte results
for status, row in unique_status_example.iterrows():
    print(f"Status: {status}")
    print("Statement:")
    print(textwrap.fill(row['statement'], width=80))  # Adjust width as needed
    print('-' * 80)  # Separator for readability

"""# **Using Natural Language Processing (NLP) for Sentiment Analysis**"""

#downloading the punkt tokenizer models
nltk.download('punkt_tab')
from nltk.tokenize import word_tokenize

#tokenize the text
tokens = word_tokenize(df['statement'][0])
print(tokens)


# tokenizing the entire dataset
df['tokens'] = df['statement'].apply(word_tokenize)

# Display tokenized text
print(df[['statement', 'tokens']].head())

# Proceed with Stemming, we do this to normalize the words to their base form
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

# Apply stemming
df['stemmed_tokens'] = df['tokens'].apply(lambda x: [stemmer.stem(word) for word in x])
print(df[['tokens', 'stemmed_tokens']].head())

# Lemmatization: Uses linguistic rules to reduce words to their base form
from nltk.stem import WordNetLemmatizer

# Download WordNet resources
nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()

# Apply lemmatization
df['lemmatized_tokens'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])
print(df[['tokens', 'lemmatized_tokens']].head())

"""**Using TF-IDF for Feature Extraction**"""

# using the lemmatized_tokens for feature extraction

from sklearn.feature_extraction.text import TfidfVectorizer

# Convert tokens back to sentences
df['processed_text'] = df['lemmatized_tokens'].apply(lambda x: ' '.join(x))

# Initialize the TF-IDF Vectorizer
tfidf = TfidfVectorizer(max_features=5000)  # Limit features to 5000

# Fit and transform the text
X = tfidf.fit_transform(df['processed_text'])
print("TF-IDF matrix shape:", X.shape)

print("Unique values in 'status':", df['status'].unique())
print("Value counts:\n", df['status'].value_counts())

# Remove rows with missing or empty values in 'status'
df = df[df['status'].notnull()]
df['status'] = df['status'].str.strip()  # Remove leading/trailing whitespace

# Create an instance of LabelEncoder
label_encoder = LabelEncoder()

y = label_encoder.fit_transform(df['status'])
print("Encoded labels after cleaning:", y[:5])
print("Classes:", label_encoder.classes_)  # List of unique classes

# since the encoded labels are 0 for each of the class type, I will label them manually
#Define custom labels
status_mapping = {
    'Anxiety': 3,
    'Bipolar': 1,
    'Depression': 2,
    'Normal': 0,
    'Personality disorder': 4,
    'Stress': 5,
    'Suicidal': 6
}

#Map the Labels: Apply this mapping to the status column to replace categories with numerical labels.
# Map the labels to the 'status' column
df['status_label'] = df['status'].map(status_mapping)

# Verify the mapping
print(df[['status', 'status_label']].head())

# Validate the label counts
print("Encoded value counts:")
print(df['status_label'].value_counts())


# Display unique statuses and their mapped labels
unique_status_mapping = df[['status', 'status_label']].drop_duplicates().sort_values(by='status_label')

# Print the unique mapping
print(unique_status_mapping)

"""**Splitting Data into train and test sets**"""

# splitting the data into training and testing sets to evaluate the model

from sklearn.model_selection import train_test_split

# Split data into features (X) and target (y)
X = df['statement']  # 'statement' contains the text data
y = df['status_label']  # Encoded target labels

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print("Training data size:", len(X_train))
print("Testing data size:", len(X_test))

# convert text into numerical features, using TF- IDF to convert
from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize the TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limit to 5000 features

# Fit and transform the training data, transform the testing data
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

print("TF-IDF matrix shape (training):", X_train_tfidf.shape)
print("TF-IDF matrix shape (testing):", X_test_tfidf.shape)

# Define the parameter grid for Logistic Regression
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength
    'max_iter': [200, 500, 1000],  # Number of iterations
    'solver': ['liblinear', 'lbfgs'],  # Optimization solvers
}

# Initialize the Logistic Regression model
from sklearn.linear_model import LogisticRegression # Import the LogisticRegression class
clf = LogisticRegression()

from sklearn.model_selection import GridSearchCV # Import GridSearchCV
# Perform GridSearchCV to find the best parameters
grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)
grid_search.fit(X_train_tfidf, y_train)

# Get best parameters and best accuracy score
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test_tfidf)

# Evaluate model performance
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)

# Print results
print("Best Parameters:", best_params)
print("Best Accuracy:", accuracy)
print("Classification Report:\n", report)

"""**Training Classification model using Logistic Regression**"""

# Train a classification model, using Logistic Regression

from sklearn.linear_model import LogisticRegression

# Initialize the Logistic Regression model
clf = LogisticRegression(max_iter=200)

# Train the model
clf.fit(X_train_tfidf, y_train)

"""# **Clf Model Evaluation**"""

# Evaluate the model 'clf' by using performance metrics like accuracy, precision, recall, F1-score and confusion matrix
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Make predictions on the test set
y_pred = clf.predict(X_test_tfidf)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Classification report
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))  # Adjust figure size for readability
sns.heatmap(
    cm,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=label_encoder.classes_,  # Use class names as labels
    yticklabels=label_encoder.classes_
)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

# Compute ROC curve and AUC for each class
from sklearn.metrics import roc_curve, auc # Import roc_curve and auc
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))

n_classes = len(label_encoder.classes_)
# Get predicted probabilities (you might need to adjust this based on your model)
y_pred_proba = clf.predict_proba(X_test_tfidf)

for i in range(n_classes):
    fpr, tpr, _ = roc_curve(y_test == i, y_pred_proba[:, i])  # Compute ROC for each class
    roc_auc = auc(fpr, tpr)  # Compute AUC
    plt.plot(fpr, tpr, label=f'{label_encoder.classes_[i]} (AUC = {roc_auc:.2f})')

# Plot the diagonal line (random classifier)
plt.plot([0, 1], [0, 1], 'k--')

# Set labels and title
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Multi-Class ROC Curve')
plt.legend(loc='lower right')
plt.show()

"""# **Using the model to make prediction**"""

# Make prediction using the same sample text for suicide saved as content

# input is in a list format for the vectorizer
content = ["I'm tired of everything. Life feels empty, and I don't see the point of continuing. I just want all this pain to stop forever."]

# Transform the new text using the TF-IDF vectorizer
new_text_tfidf = tfidf_vectorizer.transform(content)

# Predict the category
predicted_label = clf.predict(new_text_tfidf)
print("Predicted Label (numerical):", predicted_label[0])

# Convert the numerical label back to the original category using the mapping
predicted_status = [key for key, value in status_mapping.items() if value == predicted_label[0]]
print("Predicted Status:", predicted_status[0])

"""# **MODEL DEPLOYMENT**"""

# Saving the trained model

import pickle

# Save the trained model
with open("logistic_regression_model.pkl", "wb") as model_file:
    pickle.dump(best_model, model_file)

# Save the TF-IDF vectorizer
with open("tfidf_vectorizer.pkl", "wb") as vectorizer_file:
    pickle.dump(tfidf_vectorizer, vectorizer_file)

# Save the Label Encoder
with open("label_encoder.pkl", "wb") as label_encoder_file:
    pickle.dump(label_encoder, label_encoder_file)

print("Model, TF-IDF vectorizer, and label encoder saved successfully!")

# DEPLOY MODEL AS AN API
# Load the trained model, vectorizer, and label encoder
with open("logistic_regression_model.pkl", "rb") as model_file:
    model = pickle.load(model_file)

with open("tfidf_vectorizer.pkl", "rb") as vectorizer_file:
    vectorizer = pickle.load(vectorizer_file)

with open("label_encoder.pkl", "rb") as label_encoder_file:
    label_encoder = pickle.load(label_encoder_file)

!pip install fastapi
from fastapi import FastAPI

# Initialize FastAPI
app = FastAPI(title="Mental Health Classification API")

!pip install pydantic
from pydantic import BaseModel # Import BaseModel from pydantic

# Define a request model
class TextInput(BaseModel):
    text: str

# Define a route for predictions
@app.post("/predict")
def predict(data: TextInput):
    text = [data.text]

    # Transform text using the saved TF-IDF vectorizer
    text_tfidf = vectorizer.transform(text)

    # Make prediction
    predicted_label = model.predict(text_tfidf)
    predicted_class = label_encoder.inverse_transform(predicted_label)[0]

    return {"input_text": data.text, "predicted_class": predicted_class}